{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will be illustrating how to set up and use (kNN) Topological PCA for Single Cell RNA-Seq Data Analysis. We will first define all the steps behind tPCA, and then illustrate its use for cell type identification by clustering. The mathematical formulation of Topological PCA is \n",
    "\n",
    "$\\min_{U,Q}\\|X - UQ^T\\|_{2,1} + \\beta\\|Q\\|_{2,1} + \\gamma \\text{Tr}(Q^T(L_{\\rm P})Q), \\quad \\text{s.t. } Q^TQ = I_m$\n",
    "\n",
    "The first term, $\\min_{U,Q}\\|X - UQ^T\\|_{2,1}$ is simply a robust variation of principal component analysis, where we are trying to approximate the gene expression matrix $X$ by low rank matrices $U$ and $Q$. \n",
    "\n",
    "The second term $\\beta\\|Q\\|_{2,1}$ tries to enforce that the resulting embedded expression profiles are sparse, and this makes the result more interpretable due to the high sparsity of true gene expression data. $\\beta$ is a parameter that controls how sparse we want the solution to be. \n",
    "\n",
    "The last term $\\gamma \\text{Tr}(Q^T(L_{\\rm P})Q)$ is a graph embedding term, where we are seeking to preserve the cell-cell expression profile correlation relations described in the graph Laplacian term $L_p$. The idea is that if two cells have correlated expression profiles (and so are connected in the graph), then their embeddings should be located close to one another. \n",
    "\n",
    "The last condition, $Q^TQ = I_m$ is a constraint stating that $Q$ should be an orthogonal matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scanpy as sc\n",
    "from mclustpy import mclustpy\n",
    "\n",
    "def gaussian_kernel(dist, t):\n",
    "    '''\n",
    "    gaussian kernel function for weighted edges\n",
    "    '''\n",
    "    return np.exp(-(dist**2 / t))\n",
    "\n",
    "def Eu_dis(x):\n",
    "    \"\"\"\n",
    "    Calculate the distance among each raw of x\n",
    "    :param x: N X D\n",
    "                N: number of samples\n",
    "                D: Dimension of the feature\n",
    "    :return: N X N distance matrix\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    aa = np.sum(np.multiply(x, x), 1)\n",
    "    ab = x @ x.T\n",
    "    dist_mat = aa + aa.T - 2 * ab\n",
    "    dist_mat[dist_mat < 0] = 0\n",
    "    dist_mat = np.sqrt(dist_mat)\n",
    "    dist_mat = np.maximum(dist_mat, dist_mat.T)\n",
    "    dist_mat = np.asarray(dist_mat)\n",
    "    return dist_mat\n",
    "\n",
    "def cal_weighted_adj(data, n_neighbors, t):\n",
    "    '''\n",
    "    Calculate weighted adjacency matrix based on KNN\n",
    "    For each row of X, put an edge between nodes i and j\n",
    "    If nodes are among the n_neighbors nearest neighbors of each other\n",
    "    according to Euclidean distance\n",
    "    '''\n",
    "    dist = Eu_dis(data)\n",
    "    n = dist.shape[0]\n",
    "    gk_dist = gaussian_kernel(dist, t)\n",
    "    W_L = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        index_L = np.argsort(dist[i, :])[1:1 + n_neighbors] \n",
    "        len_index_L = len(index_L)\n",
    "        for j in range(len_index_L):\n",
    "            W_L[i, index_L[j]] = gk_dist[i, index_L[j]] #weighted edges\n",
    "    W_L = np.maximum(W_L, W_L.T)\n",
    "    return W_L\n",
    "\n",
    "def cal_unweighted_adj(data, n_neighbors):\n",
    "    '''\n",
    "    Calculate unweighted adjacency matrix based on KNN\n",
    "    '''\n",
    "    dist = Eu_dis(data)\n",
    "    n = dist.shape[0]\n",
    "    W_L = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        index_L = np.argsort(dist[i, :])[1:1 + n_neighbors]\n",
    "        len_index_L = len(index_L)\n",
    "        for j in range(len_index_L):\n",
    "            W_L[i, index_L[j]] = 1 #edges not weighted\n",
    "    W_L = np.maximum(W_L, W_L.T)\n",
    "    return W_L\n",
    "\n",
    "def cal_laplace(adj):\n",
    "    N = adj.shape[0]\n",
    "    D = np.zeros_like(adj)\n",
    "    for i in range(N):\n",
    "        D[i, i] = np.sum(adj[i]) # Degree Matrix\n",
    "    L = D - adj  # Laplacian\n",
    "    return L\n",
    "\n",
    "\n",
    "def tPCA_Algorithm(xMat,laplace,beta,gamma,k,n):\n",
    "    '''\n",
    "    Optimization Algorithm of tPCA \n",
    "    Solve approximately via ADMM\n",
    "    Need to compute optimal principal directions matrix U\n",
    "    Projected Data matrix Q\n",
    "    Error term matrix E = X - UQ^T\n",
    "    Z matrix used to solve Q (see supplementary information)\n",
    "\n",
    "    Inputs are data matrix X, laplacian, scale parameters, \n",
    "    number of reduced dimensions, number of original dimensions\n",
    "    '''\n",
    "    # Initialize thresholds, matrices\n",
    "    obj1 = 0\n",
    "    obj2 = 0\n",
    "    thresh = 1e-50\n",
    "    V = np.eye(n) \n",
    "    vMat = np.asarray(V) # Auxillary matrix to optimize L2,1 norm\n",
    "    E = np.ones((xMat.shape[0],xMat.shape[1]))\n",
    "    E = np.asarray(E) # Error term X - UQ^T\n",
    "    C = np.ones((xMat.shape[0],xMat.shape[1]))\n",
    "    C = np.asarray(C) # Lagrangian Multiplier\n",
    "    laplace = np.asarray(laplace) #Lplacian\n",
    "    miu = 1 #Penalty Term\n",
    "    for m in range(0, 30):\n",
    "        Z = (-(miu/2) * ((E - xMat + C/miu).T @ (E - xMat + C/miu))) + beta * vMat + gamma * laplace\n",
    "        # cal Q (Projected Data Matrix)\n",
    "        Z_eigVals, Z_eigVects = np.linalg.eig(np.asarray(Z))\n",
    "        eigValIndice = np.argsort(Z_eigVals)\n",
    "        n_eigValIndice = eigValIndice[0:k]\n",
    "        n_Z_eigVect = Z_eigVects[:, n_eigValIndice]\n",
    "        # Optimal Q given by eigenvectors corresponding\n",
    "        # to smallest k eigenvectors\n",
    "        Q = np.array(n_Z_eigVect)  \n",
    "        # cal V \n",
    "        q = np.linalg.norm(Q, ord=2, axis=1)\n",
    "        qq = 1.0 / (q * 2)\n",
    "        VV = np.diag(qq)\n",
    "        vMat = np.asarray(VV)\n",
    "        qMat = np.asarray(Q)\n",
    "        # cal U (Principal Directions)\n",
    "        U = (xMat - E - C/miu) @ qMat\n",
    "        # cal P (intermediate step)\n",
    "        P = xMat - U @ qMat.T - C/miu\n",
    "        # cal E (Error Term)\n",
    "        for i in range(E.shape[1]):\n",
    "            E[:,i] = (np.max((1 - 1.0 / (miu * np.linalg.norm(P[:,i]))),0)) * P[:,i]\n",
    "        # update C \n",
    "        C = C + miu * (E - xMat + U @ qMat.T)\n",
    "        # update miu\n",
    "        miu = 1.2 * miu\n",
    "\n",
    "        obj1 = np.linalg.norm(qMat)\n",
    "        if m > 0:\n",
    "            diff = obj2 - obj1\n",
    "            if diff < thresh:\n",
    "                break # end iterations if error within accepted threshold\n",
    "        obj2 = obj1\n",
    "    return U #return principal directions matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to introduce filtration. The idea here is that different scales of gene expression correlations may correspond to different scales of cell interactions which we want to incorporate into the model. Alternatively, we may view this objective function as trying to preserve the intrinsic geometry of the data (the graph relations). Filtration allows us to try to preserve that geometry as viewed over multiple scales, making the model more robust. \n",
    "\n",
    "We can do this by including several steps of a filtration, where at each step we change the number of neighbors we are using to construct our $kNN$ cell graph. This allows us to formulate the $k$th graph Laplacian\n",
    "\n",
    "$ L^k = (l_{ij}^k)\\text{, } l_{ij}^k = \n",
    "    \\begin{cases}\n",
    "    -1,\\text{ if } i \\neq j \\text{ and }\\mathbf{x}_j \\in \\mathcal{N}_k(\\mathbf{x}_i) \\\\\n",
    "    0, \\text{ otherwise}\n",
    "    \\end{cases}$\n",
    "\n",
    "where $l_{ii}^k = -\\sum_{j=1}^nl_{ij}^k$\n",
    "\n",
    "We can then construct what we call an Accumulated Spectral Graph where we weight each Laplacian by some value $\\zeta$ and sum them into a single term denoted $L_{\\rm P}$. \n",
    "\n",
    "$L_{\\rm P} := \\sum_{k=1}^p\\zeta_kL^k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cal_persistence_KNN(data, n_filtrations, zetas):\n",
    "    n = data.shape[0]\n",
    "    '''\n",
    "    Consider n neighbors and reduce by 2 neighbors at\n",
    "    each iteration of filtration down to 6 nearest neighbors\n",
    "    (4 filtrations)\n",
    "    '''\n",
    "    num_neighbors_list = range(6, n_filtrations + 1, 3)\n",
    "    num_filtrations = len(num_neighbors_list)\n",
    "\n",
    "    PL = np.zeros((num_filtrations, n, n))\n",
    "    zetas = np.array(zetas)\n",
    "\n",
    "    for idx, num_neighbors in enumerate(num_neighbors_list):\n",
    "        A = cal_unweighted_adj(data, num_neighbors)\n",
    "        #print('num neighbors:', num_neighbors)\n",
    "        PL[idx, :, :] = cal_laplace(A)\n",
    "        #print(\"i'th PL:\", PL[idx, :, :])\n",
    "        #print(\"zeta_i:\", zetas[idx])\n",
    "\n",
    "    Persistent_Laplacian = np.sum(zetas[:, np.newaxis, np.newaxis] * PL, axis=0)\n",
    "    return Persistent_Laplacian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit this model to the gene expression data at some combination of scales (some $\\{ \\zeta_i \\}$ combination in the $PL$ term) We use $\\zeta$ values of either 0 or 1 which corresponds to either 'turning on or off' that scale of graph connectivity in the graph embedding. Note that as the orthogonality constraint is no longer on $U$, we actually transform $X$ by the pseudo-inverse of $U$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tPCA_cal_projections_KNN(X_data, beta1, gamma1, k_d, num_filtrations, zeta):\n",
    "    n = len(X_data)  \n",
    "    M = cal_persistence_KNN(X_data, num_filtrations, zeta)\n",
    "    Y = tPCA_Algorithm(X_data.transpose(), M, beta1, gamma1, k_d, n)\n",
    "    return Y\n",
    "\n",
    "#Embed gene expression data\n",
    "def tPCA_embedding(X, beta, gamma, k, zeta):\n",
    "    #Principal Components\n",
    "    PDM = tPCA_cal_projections_KNN(X, beta, gamma, k, 15, zeta)\n",
    "    PDM = np.asarray(PDM)\n",
    "    #print(PDM.shape)\n",
    "    TM = ((np.linalg.inv(PDM.T @ PDM)) @ (PDM.T)).T\n",
    "    #Projected Data Matrix\n",
    "    Q = (X @ TM)\n",
    "    #print(Q.shape)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad('/mnt/home/cottre61/GFP-GAT/STAGATE_pyG/GraphTransformer/Data/GSE67835.h5ad')\n",
    "adata.var_names_make_unique()\n",
    "sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=3000)\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "adata = adata[:, adata.var['highly_variable']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1e2 # graph strength\n",
    "beta = 1e1 # sparsity\n",
    "k = 20 # embedding dim\n",
    "zeta = [1,0,1,0] # connectivity scales to consider (corresponding to k = 6,8,10,12)\n",
    "\n",
    "X = adata.X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]:                    __           __ \n",
      "   ____ ___  _____/ /_  _______/ /_\n",
      "  / __ `__ \\/ ___/ / / / / ___/ __/\n",
      " / / / / / / /__/ / /_/ (__  ) /_  \n",
      "/_/ /_/ /_/\\___/_/\\__,_/____/\\__/   version 6.0.1\n",
      "Type 'citation(\"mclust\")' for citing this R package in publications.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting ...\n",
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "Q = tPCA_embedding(X, beta,gamma,k,zeta)\n",
    "res = mclustpy(Q, G=7, modelNames='EEE', random_seed=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can cluster over multiple different scales, and then utilize those multiple clusterings to define a consensus clustering. We will explore this idea in MCIST. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GATE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
